---
title: "Caso Analítica de Marketing"
author: "Christian Vasquez Hernandez"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Práctica de Analítica de Marketing: Selección de Potenciales

El objetivo de esta práctica es seleccionar un conjunto de clientes que serán el objetivo
de la próxima campaña comercial de venta de seguros de ahorro.

Para ello se aporta la siguiente documentación:

* Fichero ("Campaña Historica Seguro Ahorro.csv") con resultados históricos.
* Fichero ("Campaña Nueva Seguro de Ahorro.csv") con clientes actuales de la
entidad de los que se quiere extraer un conjunto de potenciales.

Como datos significativos indicar que cada llamada a un cliente tiene un coste de 8
euros y la venta de un seguro de ahorro reporta un beneficio de 10 euros.

##Entregables

Para presentar la campaña será necesario aportar la siguiente documentación:

La selección de potenciales tiene que realizarse con el objetivo de maximizar el
beneficio de la campaña.

* Script de r con el procedimiento realizado para extraer los potenciales.
* Fichero en formato Word o PDF con la explicación de los procedimientos realizados y las decisiones tomadas.
* Fichero ("Campaña Nueva Seguro de Ahorro.csv") con las probabilidades asignadas a cada cliente en una variable llamada PROBABILIDAD y una variable llamada POTENCIAL con valor 1 si el cliente es seleccionado para la campaña y 0 si el cliente no es seleccionado.

##Instrucciones y observaciones

La práctica se deberá realizar utilizando R como herramienta.

Se puede utilizar cualquier técnica de clasificación aprendida durante el Master.

Al final de la práctica se publicará un ranking con los resultados de las campañas de los
alumnos ordenado por el beneficio obtenido en la campaña.



# Solución

Para la ejecución de este documento es importante que estén cargadas las librerias ROCR y CaTools mediante los comandos library(ROCR) y library(caTools)

```{r librerias, include=FALSE}
library(ROCR)
library(caTools)
library(randomForest)
library(xgboost)

```

Fijamos el directorio de trabajo en la carpeta donde esté contenido el fichero "Campaña Historica Seguro Ahorro.csv" y "Campaña Nueva Seguro de Ahorro.csv" mediante el comando setwd.

```{r setwd, include=FALSE}
setwd("/Users/chvasquez/Christian/Cursos/Maestria/trabajosm_eoi/Modulo15")
```

Comenzamos con los carga del dataset en dos dataframe uno con informacion historica y otra con la informacion nueva.

```{r cargadatos}
df_segurohistorica=read.csv2("Campaña Historica Seguro Ahorro.csv")
df_seguronueva=read.csv2("Campaña Nueva Seguro Ahorro.csv")
```


```{r transformacionhist}
df_segurohistorica$CAMP_DEPOSITOS=as.factor(df_segurohistorica$CAMP_DEPOSITOS)
```

### Revisamos dataset y variables

Comenzamos analizando la estructura del dataset, y las variables que tienen:

__Información historica__
```{r strHistorico}
str(df_segurohistorica)
```

Los primeros 6 registros del dataset para tener una primera impresión del contenido del dataset.
```{r headHistorico}
head(df_segurohistorica)
```

Los ultimos 6 registros del dataset para tener una primera impresión del contenido del dataset.
```{r tailHistorico}
tail(df_segurohistorica)
```

__Información Nueva__
```{r strNuevo}
str(df_seguronueva)
```

Los primeros 6 registros del dataset para tener una primera impresión del contenido del dataset.
```{r headNuevo}
head(df_seguronueva)
```

Los ultimos 6 registros del dataset para tener una primera impresión del contenido del dataset.
```{r tailNuevo}
tail(df_seguronueva)
```


Revisando los valores estadísticos de las variables para conocer de forma breve características básicas de los datos con los que estamos trabajando.

```{r summaryhist}
summary(df_segurohistorica)
```



```{r summarynueva}
summary(df_seguronueva)
```


### Bloque de creación de conjuntos de entrenamiento y test

Por este motivo dividimos nuestro conjunto de datos en tres conjuntos: entrenamiento(70%), validación(15%) y test(15%). Para que todo el proceso sea reproducible se ha fijado una semilla mediante el comando set.seed.

Primero se divide el conjunto total en dos conjuntos:

```{r split}
set.seed(1234) 
SAMPLE = sample.split(df_segurohistorica$CAMP_DEPOSITOS, SplitRatio = .70)
df_segurohistoricaTrain = subset(df_segurohistorica, SAMPLE == TRUE)
df_segurohistoricaValTest = subset(df_segurohistorica, SAMPLE == FALSE)

SAMPLE = sample.split(df_segurohistoricaValTest$CAMP_DEPOSITOS, SplitRatio = .50)
df_segurohistoricaVal= subset(df_segurohistoricaValTest, SAMPLE == TRUE)
df_segurohistoricaTest = subset(df_segurohistoricaValTest, SAMPLE == FALSE)

```

De forma que tenemos 3 conjuntos df_segurohistoricaTrain, df_segurohistoricaVal y df_segurohistoricaTest con 26'250, 5'624 y 5'624 registros respectivamente. 

```{r tamanoTodo}
dim(df_segurohistorica)
dim(df_segurohistoricaTrain)
dim(df_segurohistoricaVal)
dim(df_segurohistoricaTest)
```


El uso del comando sample.split de la librería CaTools nos permite mantener el porcentaje de éxitos por conjunto en el 31.29% aproximadamente como podemos comprobar:

```{r prior}
table(df_segurohistorica$CAMP_DEPOSITOS)
sum(df_segurohistorica$CAMP_DEPOSITOS==1)/length(df_segurohistorica$CAMP_DEPOSITOS)
table(df_segurohistoricaTrain$CAMP_DEPOSITOS)
prior=sum(df_segurohistoricaTrain$CAMP_DEPOSITOS==1)/length(df_segurohistoricaTrain$CAMP_DEPOSITOS)
prior

table(df_segurohistoricaVal$CAMP_DEPOSITOS)
sum(df_segurohistoricaVal$CAMP_DEPOSITOS==1)/length(df_segurohistoricaVal$CAMP_DEPOSITOS)

table(df_segurohistoricaTest$CAMP_DEPOSITOS)
sum(df_segurohistoricaTest$CAMP_DEPOSITOS==1)/length(df_segurohistoricaTest$CAMP_DEPOSITOS)
```


### Bloque de análisis del poder predictivo de las variables

En este bloque vamos a analizar la capacidad predictiva individual univariable de cada variable. Esto nos permite conocer qué factores son los que afectan en la contratación del producto.

Comenzamos mediante un gráfico que nos permite comparar el porcentaje de éxito de la campaña para las diferentes categorías de una variable (barras) y compararlas con el prior del dataset (línea roja). Para ello creamos la siguiente función:

```{r relevancia}
relevancia=function(Target,VariableCategorica){
  levels=levels(VariableCategorica)
  colors=c()
  for (i in 1:length(levels)){
    TABLA=table(Target,VariableCategorica==levels[i])
    chi=chisq.test(TABLA)
    if (chi$p.value<0.05){
      colors=c(colors,"green")
    }else{
      colors=c(colors,"gray")
    }
  }
  TABLA=table(Target,VariableCategorica)
  plot=barplot(100*TABLA[2,]/(TABLA[1,]+TABLA[2,]),ylim=c(0,100),col=colors,cex.names=0.6)
  text(x=plot, y=5+100*TABLA[2,]/(TABLA[1,]+TABLA[2,]),labels=paste(round(100*TABLA[2,]/(TABLA[1,]+TABLA[2,]),2),"%",sep=""))
  abline(h=100*prior,col="red")
}
```


```{r woeiv}
woe_iv=function(Target,VariableCategorica){
  Tabla_WOE=table(VariableCategorica,Target)
  DF_WOE=data.frame(FRACASOS=Tabla_WOE[,1],EXITOS=Tabla_WOE[,2])
  DF_WOE$EXITOS_PORC=DF_WOE$EXITOS/sum(DF_WOE$EXITOS)
  DF_WOE$FRACASOS_PORC=DF_WOE$FRACASOS/sum(DF_WOE$FRACASOS)
  DF_WOE$WOE=log(DF_WOE$EXITOS_PORC/DF_WOE$FRACASOS_PORC)
  DF_WOE$IV=(DF_WOE$EXITOS_PORC-DF_WOE$FRACASOS_PORC)*DF_WOE$WOE
  DF_WOE
}
```


Relevancia por Edad
```{r graph_edad}
relevancia(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$CAT_EDAD)

WOE_CAT_EDAD=woe_iv(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$CAT_EDAD)
WOE_CAT_EDAD
IV_CAT_EDAD=sum(WOE_CAT_EDAD$IV)
IV_CAT_EDAD
```


Relevancia por Estado civil
```{r graph_married}
relevancia(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$ESTADO_CIVIL)

WOE_ESTADO_CIVIL=woe_iv(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$ESTADO_CIVIL)
WOE_ESTADO_CIVIL
IV_ESTADO_CIVIL=sum(WOE_ESTADO_CIVIL$IV)
IV_ESTADO_CIVIL

```

Relevancia por NIVEL ESTUDIOS
```{r graph_study}
relevancia(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$NIVEL_ESTUDIOS)

WOE_NIVEL_ESTUDIOS=woe_iv(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$NIVEL_ESTUDIO)
WOE_NIVEL_ESTUDIOS
IV_NIVEL_ESTUDIOS=sum(WOE_NIVEL_ESTUDIOS$IV)
IV_NIVEL_ESTUDIOS

```

Relevancia por RANGO INGRESOS
```{r graph_salary}
relevancia(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$RANGO_INGRESOS)

WOE_RANGO_INGRESOS=woe_iv(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$RANGO_INGRESOS)
WOE_RANGO_INGRESOS
IV_RANGO_INGRESOS=sum(WOE_RANGO_INGRESOS$IV)
IV_RANGO_INGRESOS
```

Relevancia por SEXO
```{r graph_gender}
relevancia(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$SEXO)

WOE_SEXO=woe_iv(df_segurohistoricaTrain$CAMP_DEPOSITOS,df_segurohistoricaTrain$SEXO)
WOE_SEXO
IV_SEXO=sum(WOE_SEXO$IV)
IV_SEXO
```

Podemos establecer un ranking de capacidad predictiva ordenando las variables en función a su IV:

```{r ivrank}
IV_CAT_EDAD
IV_NIVEL_ESTUDIOS
IV_SEXO
IV_ESTADO_CIVIL
IV_RANGO_INGRESOS
```

### Bloque de Construcción de Modelos

Comenzamos con el modelo más sencillo que incluye una sola variable independiente. Hemos seleccionado la variable CAT_EDAD al ser la de mayor IV.

### Modelo Regresion Logistica:

```{r modelo1}
print("Modelo 1")
modelo1=glm(CAMP_DEPOSITOS~CAT_EDAD, data=df_segurohistoricaTrain[,-1],family=binomial(link="logit"))
summary(modelo1)

print("Modelo 2")
modelo2=glm(CAMP_DEPOSITOS~CAT_EDAD+NIVEL_ESTUDIOS, data=df_segurohistoricaTrain[,-1],family=binomial(link="logit"))
summary(modelo2)

print("Modelo 3")
modelo3=glm(CAMP_DEPOSITOS~CAT_EDAD+NIVEL_ESTUDIOS+SEXO, data=df_segurohistoricaTrain[,-1],family=binomial(link="logit"))
summary(modelo3)

print("Modelo 4")
modelo4=glm(CAMP_DEPOSITOS~CAT_EDAD+NIVEL_ESTUDIOS+SEXO+ESTADO_CIVIL, data=df_segurohistoricaTrain[,-1],family=binomial(link="logit"))
summary(modelo4)

print("Modelo 5")
modelo5=glm(CAMP_DEPOSITOS~., data=df_segurohistoricaTrain[,-1],family=binomial(link="logit"))
summary(modelo5)

```


Podemos comprobar en el último modelo que para la variable sexo, ambas categorías tienen el mismo comportamiento por lo que no mejora la capacidad predictiva del modelo y no debería incluirse.

De hecho si calculamos los AIC y BIC de todos los modelos podemos comprobar que el modelo4 es el que tiene los valores más bajos para ambas métricas por lo que es el que se ajusta mejor a los datos.

```{r aicbic}
AIC(modelo1)
AIC(modelo2)
AIC(modelo3)
AIC(modelo4)
AIC(modelo5)

BIC(modelo1)
BIC(modelo2)
BIC(modelo3)
BIC(modelo4)
BIC(modelo5)
```

Las métricas AIC y BIC equilibran la ganancia de información al meter una nueva variable con la pérdida por complicar el modelo de forma que previenen la inclusión de factores que no aporten capacidad predictiva.



### Modelo Random Forest:

Calculando modelos, en base a lo trabajado anteriormente, ajustamos en base a las variables del modelo 4 y modelo 5
```{r randomForest}
#print("Modelo 1")
#modeloRF1=randomForest(CAMP_DEPOSITOS~CAT_EDAD, data=df_segurohistoricaTrain[,-1],importance = TRUE, maxnodes=10,mtry=1,ntree=10)
#summary(modeloRF1)

#print("Modelo 2")
#modeloRF2=randomForest(CAMP_DEPOSITOS~CAT_EDAD+NIVEL_ESTUDIOS, data=df_segurohistoricaTrain[,-1],importance = TRUE, maxnodes=10,mtry=2,ntree=10)
#summary(modeloRF2)

#print("Modelo 3")
#modeloRF3=randomForest(CAMP_DEPOSITOS~CAT_EDAD+NIVEL_ESTUDIOS+SEXO, data=df_segurohistoricaTrain[,-1],importance = TRUE, maxnodes=10,mtry=3,ntree=10)
#summary(modeloRF3)

print("Modelo 4")
modeloRF4=randomForest(CAMP_DEPOSITOS~CAT_EDAD+NIVEL_ESTUDIOS+SEXO+ESTADO_CIVIL, data=df_segurohistoricaTrain[,-1],importance = TRUE, maxnodes=60,mtry=4,ntree=70)
summary(modeloRF4)

print("Modelo 5")
modeloRF5=randomForest(CAMP_DEPOSITOS~., data=df_segurohistoricaTrain[,-1],importance = TRUE, maxnodes=50,mtry=5,ntree=50)
summary(modeloRF5)
```


### Bloque de evaluación y selección de modelo.


### Validando su Accuracy del modelo Random Forest

```{r}
accuracies <-c()

#prediccion=predict(modeloRF1,df_segurohistoricaTrain)
#t<-table(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS)
#print(t)
#df_segurohistoricaTrain$Rf1 <- prediccion == df_segurohistoricaTrain$CAMP_DEPOSITOS
#accuracy <- sum(as.numeric(df_segurohistoricaTrain$Rf1))/nrow(df_segurohistoricaTrain)
#accuracies <- c(accuracies,accuracy)
#print(accuracy)

#accuracies <-c()
#prediccion=predict(modeloRF2, df_segurohistoricaTrain)
#t<-table(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS)
#print(t)
#df_segurohistoricaTrain$Rf2 <- prediccion == df_segurohistoricaTrain$CAMP_DEPOSITOS
#accuracy <- sum(as.numeric(df_segurohistoricaTrain$Rf2))/nrow(df_segurohistoricaTrain)
#accuracies <- c(accuracies,accuracy)
#print(accuracy)

#prediccion=predict(modeloRF3,df_segurohistoricaTrain)
#t<-table(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS)
#print(t)
#df_segurohistoricaTrain$Rf3 <- prediccion == df_segurohistoricaTrain$CAMP_DEPOSITOS
#accuracy <- sum(as.numeric(df_segurohistoricaTrain$Rf3))/nrow(df_segurohistoricaTrain)
#accuracies <- c(accuracies,accuracy)
#print(accuracy)

prediccion=predict(modeloRF4,df_segurohistoricaTrain)
t<-table(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS)
print(t)
df_segurohistoricaTrain$Rf4 <- prediccion == df_segurohistoricaTrain$CAMP_DEPOSITOS
accuracy <- sum(as.numeric(df_segurohistoricaTrain$Rf4))/nrow(df_segurohistoricaTrain)
accuracies <- c(accuracies,accuracy)
print(accuracy)

prediccion=predict(modeloRF5,df_segurohistoricaTrain)
t<-table(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS)
print(t)
df_segurohistoricaTrain$Rf5 <- prediccion == df_segurohistoricaTrain$CAMP_DEPOSITOS
accuracy <- sum(as.numeric(df_segurohistoricaTrain$Rf5))/nrow(df_segurohistoricaTrain)
accuracies <- c(accuracies,accuracy)
print(accuracy)

```



### Validando su Accuracy del modelo regression logistica

Para evaluar y seleccionar un modelo vamos a utilizar una métrica ampliamente aceptada en los problemas de clasificación se utilice la técnica que se utilice: AUC (Area Under Curve) o área bajo la curva.

Para el primer modelo vemos que el AUC en entrenamiento 0,7020 es similar al AUC en validación 0,6986. Esto es importante puesto que nos muestra que el modelo tiene la misma capacidad predictiva en el conjunto en el que se ha entrenado que en otro conjunto del cual no se ha entrenado, confirmando que los patrones aprendidos por el modelo son generales, por lo que no se observa sobreajuste u overfitting.

```{r aucs}
prediccion=predict(modelo1,type="response")
Pred_auxiliar= prediction(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo1_train = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo1, newdata=df_segurohistoricaVal,type="response")
Pred_auxiliar = prediction(prediccion, df_segurohistoricaVal$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo1_val = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo2,type="response")
Pred_auxiliar= prediction(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo2_train = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo2, newdata=df_segurohistoricaVal,type="response")
Pred_auxiliar = prediction(prediccion, df_segurohistoricaVal$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo2_val = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo3,type="response")
Pred_auxiliar= prediction(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo3_train = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo3, newdata=df_segurohistoricaVal,type="response")
Pred_auxiliar = prediction(prediccion, df_segurohistoricaVal$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo3_val = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo4,type="response")
Pred_auxiliar= prediction(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo4_train = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo4, newdata=df_segurohistoricaVal,type="response")
Pred_auxiliar = prediction(prediccion, df_segurohistoricaVal$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo4_val = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo5,type="response")
Pred_auxiliar= prediction(prediccion, df_segurohistoricaTrain$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo5_train = as.numeric(auc.tmp@y.values)

prediccion=predict(modelo5, newdata=df_segurohistoricaVal,type="response")
Pred_auxiliar = prediction(prediccion, df_segurohistoricaVal$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo5_val = as.numeric(auc.tmp@y.values)

```

Representamos los resultados:

```{r seleccion, echo=FALSE}
Modelo_1=c(auc_modelo1_train,auc_modelo1_val)
Modelo_2=c(auc_modelo2_train,auc_modelo2_val)
Modelo_3=c(auc_modelo3_train,auc_modelo3_val)
Modelo_4=c(auc_modelo4_train,auc_modelo4_val)
Modelo_5=c(auc_modelo5_train,auc_modelo5_val)

A=data.frame(Modelo_1,Modelo_2,Modelo_3,Modelo_4,Modelo_5)
rownames(A)=c("auc_train","auc_val")
colnames(A)=c("Modelo1","Modelo2","Modelo3","Modelo4","Modelo5")
A
```

Podemos apreciar que el modelo con mayor AUC en validación es el modelo 4, por lo que este es el modelo elegido puesto que presenta los mejores resultados en el conjunto de validación. 

Una vez elegido el modelo4 sólo nos queda evaluar su capacidad y esto se realiza utilizando el conjunto de test que no ha sido utilizado en ninguna parte del proceso de construcción y selección del modelo.

```{r auctest}
df_segurohistoricaTest$prediccion=predict(modelo4, newdata=df_segurohistoricaTest,type="response")
Pred_auxiliar = prediction(df_segurohistoricaTest$prediccion, df_segurohistoricaTest$CAMP_DEPOSITOS, label.ordering = NULL)
auc.tmp = performance(Pred_auxiliar, "auc");
auc_modelo4_test = as.numeric(auc.tmp@y.values)
auc_modelo4_test
```

``` {r }
head(df_segurohistoricaTest)
```

En este caso el modelo resultante tiene un AUC de 0,7357. Gráficamente se puede representar de la siguiente manera para el conjunto de test:

```{r roctrain}
CURVA_ROC_modelo4_train <- performance(Pred_auxiliar,"tpr","fpr")
plot(CURVA_ROC_modelo4_train,colorize=TRUE)
abline(a=0,b=1,col="black")
```

Otra métrica habitual para representar la capacidad predictiva de un modelo es el Índice de Gini que se puede obtener fácilmente del AUC:

```{r gini}
GINI_train=2*auc_modelo4_train-1
GINI_train
GINI_test=2*auc_modelo4_test-1
GINI_test
```

Como el Índice de Gini es una combinación lineal positiva del AUC, se podría haber realizado la selección de modelos utilizando el Índice de Gini en lugar del AUC.

Para terminar con la valoración del modelo, podemos mostrar la capacidad del modelo de la siguiente manera:

```{r vapacidad}
mean(as.numeric(df_segurohistoricaTest$CAMP_DEPOSITOS)-1)
aggregate(df_segurohistoricaTest$prediccion~df_segurohistoricaTest$CAMP_DEPOSITOS,FUN=mean)
```

Como podemos apreciar, el éxito medio de la campaña es un 31,29%. Nuestro modelo está asignando una probabilidad media del 42,48% a aquellos que efectivamente fueron éxito de la campaña y un 26,41% a aquellos que no fueron éxito, por lo que el modelo está discriminando.

### Bloque de explotación del modelo

El modelo predictivo elegido nos asigna a cada cliente una probabilidad de éxito de la campaña, es decir, los ordena los clientes por su probabilidad de éxito de la campaña. 


```{r confusion}
ALPHA=0.4248821
Confusion_Test=table(df_segurohistoricaTest$CAMP_DEPOSITOS,df_segurohistoricaTest$prediccion>=ALPHA)
Confusion_Test
```

En este caso seleccionaríamos 1'508 clientes (2ª columna 612+896), de los que 896 serían éxito. Las métricas asociadas a esta matriz de confusión serían:

```{r confmetricas}
Accuracy_Test= (sum(df_segurohistoricaTest$CAMP_DEPOSITOS==1 & df_segurohistoricaTest$prediccion>=ALPHA)+sum(df_segurohistoricaTest$CAMP_DEPOSITOS==0 & df_segurohistoricaTest$prediccion<ALPHA))/length(df_segurohistoricaTest$CAMP_DEPOSITOS)
Precision_Test=sum(df_segurohistoricaTest$CAMP_DEPOSITOS==1 & df_segurohistoricaTest$prediccion>=ALPHA)/sum(df_segurohistoricaTest$prediccion>=ALPHA)
Cobertura_Test=sum(df_segurohistoricaTest$CAMP_DEPOSITOS==1 & df_segurohistoricaTest$prediccion>=ALPHA)/sum(df_segurohistoricaTest$CAMP_DEPOSITOS==1)
Accuracy_Test
Precision_Test
Cobertura_Test
```

Podemos ver que el acierto sería del 74,60% (acierto), pero más importante es que del conjunto seleccionado vamos a acertar en un 65,57% (precisión) y estamos llamando al 39,69% de los éxitos (cobertura).

Si modificamos el umbral a otros valores conseguimos seleccionar distintas cantidades de clientes con lo que podemos modificar los parámetros de éxito para adecuarlos a nuestras necesidades. 

### Bloque de selección de umbral como el punto de máxima discriminación

Esta técnica calcula el umbral en el que existe la máxima discriminación, la cuantificación de la discriminación es una métrica que recibe el nombre KS y también es utilizada como métrica de capacidad predictiva de un modelo principalmente en modelos de scoring.

Calculamos el punto de máxima discriminación:

```{r calculoKS}
BANK_KS=df_segurohistoricaTest[order(df_segurohistoricaTest$prediccion, decreasing=TRUE),c("CAMP_DEPOSITOS","prediccion")]
BANK_KS$N=1:length(BANK_KS$CAMP_DEPOSITOS)
BANK_KS$EXITOS_ACUM=cumsum(as.numeric(BANK_KS$CAMP_DEPOSITOS)-1)
BANK_KS$FRACASOS_ACUM=BANK_KS$N-BANK_KS$EXITOS_ACUM
BANK_KS$EXITOS_TOT=sum(BANK_KS$CAMP_DEPOSITOS==1)
BANK_KS$FRACASOS_TOT=sum(BANK_KS$CAMP_DEPOSITOS==0)
BANK_KS$TOTAL=BANK_KS$EXITOS_TOT+BANK_KS$FRACASOS_TOT
BANK_KS$TPR=BANK_KS$EXITOS_ACUM/BANK_KS$EXITOS_TOT
BANK_KS$FPR=BANK_KS$FRACASOS_ACUM/BANK_KS$FRACASOS_TOT
BANK_KS$DIFF=BANK_KS$TPR-BANK_KS$FPR
plot(BANK_KS$DIFF, xlab="",ylab="discriminación")
```

el valor de la máxima discriminación se llama KS

```{r KS}
KS=max(BANK_KS$DIFF)
KS
```

y el valor donde se alcanza este valor es el que nos va a permitir seleccionar el umbral

```{r umbralKS}
which(BANK_KS$DIFF==KS)
BANK_KS[1590,c("CAMP_DEPOSITOS","prediccion")]
```

en este caso el umbral seleccionado de máxima discriminación sería 0.3629083

### Bloque de selección de umbral como el punto óptimo estadístico (F1-Score)

Esta técnica selecciona el umbral como el punto que optimiza la relación entre la precisión y la cobertura maximizando el F1-Score o F-Score.

```{r calculoF1}
BANK_KS$Accuracy=(BANK_KS$EXITOS_ACUM+BANK_KS$FRACASOS_TOT-BANK_KS$FRACASOS_ACUM)/BANK_KS$TOTAL
BANK_KS$Precision=BANK_KS$EXITOS_ACUM/BANK_KS$N
BANK_KS$Cobertura=BANK_KS$EXITOS_ACUM/BANK_KS$EXITOS_TOT
BANK_KS$F1Score=2*(BANK_KS$Precision*BANK_KS$Cobertura)/(BANK_KS$Precision+BANK_KS$Cobertura)
plot(BANK_KS$F1Score,xlab="",ylab="F1-Score")
```
El punto donde se alcanzaría el máximo sería:

```{r F1}
max(BANK_KS$F1Score)
```

y el valor donde se alcanza este valor es el que nos va a permitir seleccionar el umbral

```{r umbralF1}
which(BANK_KS$F1Score==max(BANK_KS$F1Score))
BANK_KS[2303,c("CAMP_DEPOSITOS","prediccion")]
```

en este caso el umbral seleccionado que maximiza el F1-Score es 0.303446.


### Bloque de selección de umbral como el punto que maximiza el Beneficio de la campaña

Si consideramos un coste por llamada y un beneficio por venta

```{r costes}
costeLlamada=10
beneficioVenta=15
```

Podemos definir el beneficio o pérdida que tendrán nuestras acciones

```{r beneficios}
BANK_KS$BeneficioTP=beneficioVenta-costeLlamada
BANK_KS$BeneficioTN=0
BANK_KS$PerdidaFP=-costeLlamada
BANK_KS$PerdidaFN=-beneficioVenta
```

Con estos datos podemos calcular el beneficio financiero en función al umbral de corte para seleccionar el punto máximo que nos defina el umbral a considerar

```{r umbralBeneficio}
BANK_KS$BeneficioFinan=BANK_KS$EXITOS_ACUM*BANK_KS$BeneficioTP+
  BANK_KS$FRACASOS_ACUM*BANK_KS$PerdidaFP
plot(BANK_KS$BeneficioFinan,xlab="",ylab="Beneficio Financiero")
max(BANK_KS$BeneficioFinan)
which(BANK_KS$BeneficioFinan==max(BANK_KS$BeneficioFinan))
BANK_KS[422,c("CAMP_DEPOSITOS","prediccion")]
```

En este caso tenemos varios puntos máximos por lo que podemos seleccionar uno, eligiendo el primero, el umbral sería del 0.6921934 y hubiéramos obtenido un beneficio de 565.

Pero podemos considerar que dejar de llamar a un cliente que va a contratar supone un coste de oportunidad en el que incurrimos por lo que podemos calcular el umbral que maximice el beneficio teniendo en cuenta el coste de oportunidad:

```{r umbralOprotunidad}
BANK_KS$Oportunidad=BANK_KS$EXITOS_ACUM*BANK_KS$BeneficioTP+
  (BANK_KS$EXITOS_TOT-BANK_KS$EXITOS_ACUM)*BANK_KS$PerdidaFN+
  BANK_KS$FRACASOS_ACUM*BANK_KS$PerdidaFP+
  (BANK_KS$FRACASOS_TOT-BANK_KS$FRACASOS_ACUM)*BANK_KS$BeneficioTN
plot(BANK_KS$Oportunidad,xlab="",ylab="Beneficio - Coste de Oportunidad")
max(BANK_KS$Oportunidad)
which(BANK_KS$Oportunidad==max(BANK_KS$Oportunidad))
BANK_KS[1546,c("CAMP_DEPOSITOS","prediccion")]
```

En este caso, el umbral se fijaría en el 0.3629083 y hubiéramos obtenido una perdida de -14.425.


# Escribiendo del Resultado FINAL

___Revisando los resultados obtenidos, nos quedamos con el modelo de Regresion logistica Modelo 4, ya que tiene mejores resultados en accuracy, por lo que generamos la data propuesta para el ejercicio.___

```{r resultadofinal}
df_seguronueva$prediccion=predict(modelo4, newdata=df_seguronueva,type="response")

ALPHA=0.6921934
df_seguronueva$resultado= c(df_seguronueva$prediccion>=ALPHA)
df_seguronueva$resultado=as.integer(df_seguronueva$resultado)

head(df_seguronueva, 10)

write.csv(df_seguronueva, file = "outputmodel.csv", row.names = FALSE)

```

