{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*************************************************************************\n",
    "#1.Importamos la libreria NLTK\n",
    "#*************************************************************************\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#*************************************************************************\n",
    "#2.Instalamos todas las librerias/corpus/modelos que sean necesarios\n",
    "#*************************************************************************\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Texto: I didn't notice my animals were uglier than yours! I'm sorry...\n"
     ]
    }
   ],
   "source": [
    "#*************************************************************************\n",
    "#2.Creamos una texto de entrada a nuestra cadena NLP\n",
    "#*************************************************************************\n",
    "text = \"I didn't notice my animals were uglier than yours! I'm sorry...\"\n",
    "print (\"\\n\\n1. Texto:\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. Frases: [\"I didn't notice my animals were uglier than yours!\", \"I'm sorry...\"]\n"
     ]
    }
   ],
   "source": [
    "#*************************************************************************\n",
    "#2.Dividimos el texto en frases\n",
    "#*************************************************************************\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "print (\"\\n\\n2. Frases:\",sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "3. Tokens: ['I', 'did', \"n't\", 'notice', 'my', 'animals', 'were', 'uglier', 'than', 'yours', '!', 'I', \"'m\", 'sorry', '...']\n"
     ]
    }
   ],
   "source": [
    "#*****************************************************************************\n",
    "#3.Tokenización: tokenizamos el texto, es decir dividimos el texto en tokens\n",
    "#*****************************************************************************\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print (\"\\n\\n3. Tokens:\",tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "4. Analisis Morfologico: [('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('notice', 'VB'), ('my', 'PRP$'), ('animals', 'NNS'), ('were', 'VBD'), ('uglier', 'JJR'), ('than', 'IN'), ('yours', 'JJR'), ('!', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('sorry', 'JJ'), ('...', ':')]\n"
     ]
    }
   ],
   "source": [
    "#*************************************************************************\n",
    "#4.Análisis morfológico: asignamos una etiqueta morfologica a cada token\n",
    "#*************************************************************************\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print (\"\\n\\n4. Analisis Morfologico:\",tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "5. Stems: \n",
      "i\n",
      "did\n",
      "n't\n",
      "notic\n",
      "my\n",
      "anim\n",
      "were\n",
      "uglier\n",
      "than\n",
      "your\n",
      "!\n",
      "i\n",
      "'m\n",
      "sorri\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "#*******************************************************************  \n",
    "#5.Stemming: obtenemos la raíz (en inglés 'stem') de cada token\n",
    "#*******************************************************************  \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print (\"\\n\\n5. Stems: \")\n",
    "for tok in tokens:\n",
    "    print (stemmer.stem(tok.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "6. Lemas: \n",
      "i\n",
      "do\n",
      "not\n",
      "notice\n",
      "my\n",
      "animal\n",
      "be\n",
      "ugly\n",
      "than\n",
      "yours\n",
      "!\n",
      "i\n",
      "be\n",
      "sorry\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "#*******************************************************************  \n",
    "#6.Lematización: obtenemos el lema de cada token \n",
    "#*******************************************************************  \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#El lematizador de wordnet solo reconoce 4 etiquetas POS: a (adjetivo), r(adverbio),n (nombre),v(verbo). \n",
    "#Así que debemos hacer una conversión del formato Penn Tree Bank al formato wordnet (ej: NN->n, JJ->a, RB->r, VB->V, ...)\n",
    "from nltk.corpus import wordnet\n",
    "wnTags = {'N':wordnet.NOUN,'J':wordnet.ADJ,'V':wordnet.VERB,'R':wordnet.ADV} \n",
    "print (\"\\n\\n\\n6. Lemas: \")\n",
    "for (tok,tag) in tagged:\n",
    "    #wordnet no contiene las formas abreviadas 'm  y  n't así que las introducimos nosotros para que lematice bien\n",
    "    if tok=='\\'m':\n",
    "        tok = 'am'\n",
    "    if tok=='\\'s':\n",
    "        tok = 'is'\n",
    "    if tok=='n\\'t':\n",
    "        tok = 'not'\n",
    "    tag = tag[:1]\n",
    "    lemma = lemmatizer.lemmatize(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "    #otra forma alternativa de obtener el lema hubiera sido llamar directamente a la funcion wordnet.morphy, que hace lo mismo:\n",
    "    #lemma = wordnet.morphy(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "    if lemma is None: #Si wordnet no contiene la palabra, supondremos que el lema es igual al token\n",
    "       lemma = tok.lower() \n",
    "    print (lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "7. Analisis sintactico:\n",
      "\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pijamas))))) \n",
      "\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pijamas)))))) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#*******************************************************************    \n",
    "#7.Análisis sintáctico\n",
    "#******************************************************************* \n",
    "\n",
    "#Partimos de una frase de un conocido texto de Groucho Marx, con una clara ambigüedad: \n",
    "#\"While hunting in Africa, I shot an elephant in my pijamas. How he got into my pijamas, I don't know.\"\n",
    "#¿Groucho estaba en pijama o el elefante estaba dentro de su pijama?\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pijamas']\n",
    "\n",
    "#Creamos nuestra propia Gramatica Libre de Contexto (en inglés CFG)\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pijamas'\n",
    "V -> 'shot' | 'did'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "#Generamos un parser sintáctico capaz de reconocer la gramática\n",
    "parser = nltk.ChartParser(grammar)\n",
    "print ('\\n\\n\\n7. Analisis sintactico:\\n')\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree,'\\n')\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad del Trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomar el texto de entrada: “I didn't notice my animals were uglier than yours! I'm sorry...”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Texto: I didn't notice my animals were uglier than yours. I'm sorry...\n"
     ]
    }
   ],
   "source": [
    "#*************************************************************************\n",
    "#Creamos una texto de entrada a nuestra cadena NLP\n",
    "#*************************************************************************\n",
    "text = \"I didn't notice my animals were uglier than yours. I'm sorry...\"\n",
    "print (\"\\n1. Texto:\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizar dicho texto de entrada. En el paso 7 se tokeniza manualmente entregando un array de\n",
    "palabras al parser, ahora no debemos hacer eso sino que debemos usar un tokenizador y un divisor\n",
    "de frases, que alimenten al parser. En el paso 7 sólo se analizaba una frase, ahora nosotros debemos\n",
    "analizar con nuestro programa todas las frases que contenga nuestro texto, recorriendolas con un\n",
    "bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2. Frases: [\"I didn't notice my animals were uglier than yours.\", \"I'm sorry...\"]\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "print (\"\\n\\n2. Frases:\",sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def func_test(text) :\n",
    "    print(\"\\n************ sentence ************\\n\",text, \"\\n**********************************\")\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    print (\"\\n Tokens:\",tokens)\n",
    "    \n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    print (\"\\n Analisis Morfologico:\",tagged)\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #El lematizador de wordnet solo reconoce 4 etiquetas POS: a (adjetivo), r(adverbio),n (nombre),v(verbo). \n",
    "    #Así que debemos hacer una conversión del formato Penn Tree Bank al formato wordnet (ej: NN->n, JJ->a, RB->r, VB->V, ...)\n",
    "    from nltk.corpus import wordnet\n",
    "    wnTags = {'N':wordnet.NOUN,'J':wordnet.ADJ,'V':wordnet.VERB,'R':wordnet.ADV}\n",
    "    \n",
    "    my_lemas = []\n",
    "\n",
    "    \n",
    "    print (\"\\n Lemas: \")\n",
    "    for (tok,tag) in tagged:\n",
    "        #wordnet no contiene las formas abreviadas 'm  y  n't así que las introducimos nosotros para que lematice bien\n",
    "        if tok=='\\'m':\n",
    "            tok = 'am'\n",
    "        if tok=='\\'s':\n",
    "            tok = 'is'\n",
    "        if tok=='n\\'t':\n",
    "            tok = 'not'\n",
    "        if tok=='\\'re':\n",
    "            tok = 'are'\n",
    "        if tok=='\\'ll':\n",
    "            tok = 'will'\n",
    "        if tok=='\\'d':\n",
    "            tok = 'would'\n",
    "        if tok=='\\'ve':\n",
    "            tok = 'have'\n",
    "        tag = tag[:1]\n",
    "        lemma = lemmatizer.lemmatize(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "        #otra forma alternativa de obtener el lema hubiera sido llamar directamente a la funcion wordnet.morphy, que hace lo mismo:\n",
    "        #lemma = wordnet.morphy(tok.lower(),wnTags.get(tag,wordnet.NOUN))\n",
    "        if lemma is None: #Si wordnet no contiene la palabra, supondremos que el lema es igual al token\n",
    "           lemma = tok.lower()\n",
    "\n",
    "        my_lemas.append(lemma)\n",
    "\n",
    "    my_lemas = [x for i,x in enumerate(my_lemas) if x.isalpha() ]    \n",
    "    #my_lemas = [x for i,x in enumerate(my_lemas) if (x!='.') and (x!='...') ]    \n",
    "    print(my_lemas)\n",
    "\n",
    "    #Creamos nuestra propia Gramatica Libre de Contexto (en inglés CFG)\n",
    "    grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP | NP V RB N NP NP VP\n",
    "    SN -> NP V RB\n",
    "    PP -> P NP \n",
    "    NP -> Det N | Det N PP | 'i' | V N \n",
    "    VP -> V NP | VP PP | V JJ | V RB N| IN Det  \n",
    "    JJ -> 'sorry'\n",
    "    Det -> 'an' | 'my' | 'yours'\n",
    "    N -> 'animal' | 'notice' |'ugly'\n",
    "    V -> 'do' | 'did' |'be'  \n",
    "    RB -> 'not'\n",
    "    IN -> 'than'\n",
    "    P -> 'in'\n",
    "    \n",
    "    \"\"\")\n",
    "\n",
    "    print(grammar)\n",
    "    \n",
    "    #Generamos un parser sintáctico capaz de reconocer la gramática\n",
    "    parser = nltk.ChartParser(grammar)\n",
    "    print ('\\n\\n\\n7. Analisis sintactico:\\n')\n",
    "    \n",
    "    print(my_lemas)\n",
    "    \n",
    "    for tree in parser.parse(my_lemas):\n",
    "        print(tree,'\\n')\n",
    "        tree.draw()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escribir una nueva gramática que se adapte al nuevo texto. Pueden inventarse los símbolos\n",
    "sintácticos o ponerse en español si se prefiere: por ej SN (Sintagma Nominal) en vez de NP (Noun\n",
    "Phrase). Escribir una gramática es complejo y tedioso, por tanto es recomendable comenzar\n",
    "escribiendo una gramática muy sencilla (una sola regla), y a continuación ejecutarla para ver el\n",
    "resultado. Una vez obtenido un resultado sin error, añadir nuevas reglas, sucesivamente, una a una,\n",
    "para ser capaces de detectar los fallos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- S - Sentence\n",
    "- NP - Noun Phrase\n",
    "- VP - Verb Phrase\n",
    "- NN - Singular Noun \n",
    "- PP - Prepositional Phrase \n",
    "- DT - Determiner\n",
    "- VI - Intransitive Verb\n",
    "- VT - Transitive Verb\n",
    "- IN - Preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ sentence ************\n",
      " I didn't notice my animals were uglier than yours. \n",
      "**********************************\n",
      "\n",
      " Tokens: ['I', 'did', \"n't\", 'notice', 'my', 'animals', 'were', 'uglier', 'than', 'yours', '.']\n",
      "\n",
      " Analisis Morfologico: [('I', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('notice', 'VB'), ('my', 'PRP$'), ('animals', 'NNS'), ('were', 'VBD'), ('uglier', 'JJR'), ('than', 'IN'), ('yours', 'NNS'), ('.', '.')]\n",
      "\n",
      " Lemas: \n",
      "['i', 'do', 'not', 'notice', 'my', 'animal', 'be', 'ugly', 'than', 'yours']\n",
      "Grammar with 26 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    S -> NP V RB N NP NP VP\n",
      "    SN -> NP V RB\n",
      "    PP -> P NP\n",
      "    NP -> Det N\n",
      "    NP -> Det N PP\n",
      "    NP -> 'i'\n",
      "    NP -> V N\n",
      "    VP -> V NP\n",
      "    VP -> VP PP\n",
      "    VP -> V JJ\n",
      "    VP -> V RB N\n",
      "    VP -> IN Det\n",
      "    JJ -> 'sorry'\n",
      "    Det -> 'an'\n",
      "    Det -> 'my'\n",
      "    Det -> 'yours'\n",
      "    N -> 'animal'\n",
      "    N -> 'notice'\n",
      "    N -> 'ugly'\n",
      "    V -> 'do'\n",
      "    V -> 'did'\n",
      "    V -> 'be'\n",
      "    RB -> 'not'\n",
      "    IN -> 'than'\n",
      "    P -> 'in'\n",
      "\n",
      "\n",
      "\n",
      "7. Analisis sintactico:\n",
      "\n",
      "['i', 'do', 'not', 'notice', 'my', 'animal', 'be', 'ugly', 'than', 'yours']\n",
      "(S\n",
      "  (NP i)\n",
      "  (V do)\n",
      "  (RB not)\n",
      "  (N notice)\n",
      "  (NP (Det my) (N animal))\n",
      "  (NP (V be) (N ugly))\n",
      "  (VP (IN than) (Det yours))) \n",
      "\n",
      "\n",
      "************ sentence ************\n",
      " I'm sorry... \n",
      "**********************************\n",
      "\n",
      " Tokens: ['I', \"'m\", 'sorry', '...']\n",
      "\n",
      " Analisis Morfologico: [('I', 'PRP'), (\"'m\", 'VBP'), ('sorry', 'JJ'), ('...', ':')]\n",
      "\n",
      " Lemas: \n",
      "['i', 'be', 'sorry']\n",
      "Grammar with 26 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    S -> NP V RB N NP NP VP\n",
      "    SN -> NP V RB\n",
      "    PP -> P NP\n",
      "    NP -> Det N\n",
      "    NP -> Det N PP\n",
      "    NP -> 'i'\n",
      "    NP -> V N\n",
      "    VP -> V NP\n",
      "    VP -> VP PP\n",
      "    VP -> V JJ\n",
      "    VP -> V RB N\n",
      "    VP -> IN Det\n",
      "    JJ -> 'sorry'\n",
      "    Det -> 'an'\n",
      "    Det -> 'my'\n",
      "    Det -> 'yours'\n",
      "    N -> 'animal'\n",
      "    N -> 'notice'\n",
      "    N -> 'ugly'\n",
      "    V -> 'do'\n",
      "    V -> 'did'\n",
      "    V -> 'be'\n",
      "    RB -> 'not'\n",
      "    IN -> 'than'\n",
      "    P -> 'in'\n",
      "\n",
      "\n",
      "\n",
      "7. Analisis sintactico:\n",
      "\n",
      "['i', 'be', 'sorry']\n",
      "(S (NP i) (VP (V be) (JJ sorry))) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in sentences:\n",
    "    func_test(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dibujar los distintos análisis de cada frase, mediante el comando 'tree.draw()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir, en el programa que acabamos de ver no se llega a completar una cadena NLP ya que el\n",
    "último proceso (análisis sintáctico) no toma como entrada el texto inicial sino un nuevo texto.\n",
    "Nosotros en la práctica deberemos conseguir enlazar todos los elementos de la cadena, además de\n",
    "crear una nueva gramática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EC2 Logo](arbol1.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![EC2 Logo](arbol2.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
